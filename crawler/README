crawler.c
All code was written and tested on iOS 10.10.3
Edited with Sublime Text 2

INPUTS:
fist argument — The url to be crawled. Its prefix must be “http://old-www.cs.dartmouth.edu/~cs50/tse/“

second argument — The directory where output files will be written. Must be a valid pre-existing directory.

third argument — The crawler depth. Depth 1 will write a file for just the seed url. Depth 2 will write files for the seed url and all of the urls found in it’s html. This pattern will continue for larger depths. 


OUTPUTS:
The url currently being crawled will be printed to standard out. The html of all of the urls found with the prefix http://old-www.cs.dartmouth.edu/~cs50/tse/ will be written to a file in the directory specified by the user. These files will be numbered according to the order the urls were processed. The first line of the file is the url, the second line is the depth it was found at, and the rest of the file is the html. If multiple urls produce the same html, only one file will be written with the html. 


System Libraries:
curl/curl.h — needed to take advantage of functions in web.c that curl webpages to get their information

string.h — needed to copy, compare, and create strings. Used to check whether urls that are being found are in the correct domain, and to create strings to print to output files

sys/types.h, sys/stat.h, unistd.h — all needed for the stat() function which is used to check whether the directory the user specifies is a vlid directory

stdio.h — needed to write to the output files

stdlib.h — needed for the exit() function, and allocating memory


Local includes:
common.h — contains commonly used constants and the commonly used WebPage struct which stores information for webpages

web.h — defines three prototypes used in web.c necessary for handling the urls

list.h — defines the ListNode and List structs and three prototypes that allow for the functionality of a doubly linked list with two sentinels. These prototypes are implemented in list.c

hashtable.h — defines the HashTableNode and HashTable structs and four prototypes that allow for the functionality of a hash table using the Jenkins Hash function.

utils.h — primarily used for debugging


COMPILING:
crawler should be compiled using the makefile in the src directory. The makefile compiles the crawler.c file with the web.c, list.c hashtable.c, using ggc with the -Wall, -pedantic, -std=c11, and -lcurl options. 

Other considerations:
1. In order to prevent internal references being as counting as different web pages, and therefore avoid duplicate htmls being written to the output files, I added a small block of code provided by Saisi the TA to the NormalizeURL function in web.c.

2. The CURLOPT_FAILONERROR option is used in the GetWebPage function in web.c to handle 4xx error codes.

3. There are currently memory leaks in the code.

4. The crawlerTest.sh script runs multiple tests on a compiled crawler file. The output of a test file  redirected to a log file: crawlerTestlog.‘date +"%a_%b_%d_%T_%Y"‘ This test script uses ./testdir as a valid directory and ./baddir as an invalid directory because I use ./testdir as the directory to where I wrote the output files. If it is run in a directory where ./testdir does not exist, the output may be different from that in my log.



